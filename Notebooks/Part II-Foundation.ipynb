{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fa6db2-75fe-4089-90fa-586eb81aa58d",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4258b7f-abe2-45c2-91d5-85166280ec0a",
   "metadata": {},
   "source": [
    "## Part II: Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8d5d7-15fc-458b-b79d-5cd741d8d674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chapter 1: Data Preparation in a Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0341fa-0515-4238-87b3-a2eeb56b4711",
   "metadata": {},
   "source": [
    "#### Tutorial Overview\n",
    "- Introduction to data preparation within a broader machine learning project context.\n",
    "- Discusses the applied machine learning process and the role of data preparation.\n",
    "- Explains how to choose data preparation techniques based on project needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfcda9d-c01f-478d-a639-21666ab6cc24",
   "metadata": {},
   "source": [
    "#### Applied Machine Learning Process\n",
    "1. **Define Problem**\n",
    "   - Understanding the project goals and framing the prediction task (classification, regression, etc.).\n",
    "   - Collecting relevant data and defining the prediction problem clearly.\n",
    "2. **Prepare Data**\n",
    "   - Transforming raw data into a suitable form for modeling.\n",
    "   - Common tasks include cleaning data, selecting features, and transforming variables.\n",
    "3. **Evaluate Models**\n",
    "   - Designing a robust test harness to evaluate models.\n",
    "   - Selecting performance metrics, establishing baselines, and using resampling techniques like k-fold cross-validation.\n",
    "4. **Finalize Model**\n",
    "   - Selecting the best model based on evaluation results.\n",
    "   - Summarizing model performance and integrating the model into a production system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f8e94-af8f-4b60-b74a-11ea49279307",
   "metadata": {},
   "source": [
    "#### What Is Data Preparation\n",
    "- Transforming raw data to meet the requirements of machine learning algorithms.\n",
    "- Includes tasks like data cleaning, feature selection, data transforms, feature engineering, and dimensionality reduction.\n",
    "- Aims to expose the underlying structure of the problem to learning algorithms.\n",
    "- Use because:\n",
    "    + Machine learning algorithms require data to be numbers\n",
    "    + Some machine learning algorithms impose requirements on the data.\n",
    "    + Statistical noise and errors in the data may need to be corrected.\n",
    "    + Complex nonlinear relationships may be teased out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278452e3-bb96-4b55-8a64-3c12d43890fc",
   "metadata": {},
   "source": [
    "#### How to Choose Data Preparation Techniques\n",
    "- Informed by the steps before and after data preparation in the project.\n",
    "- Analysis of collected data, including summary statistics and data visualization, helps determine necessary transformations.\n",
    "- The choice of algorithms and evaluation metrics also guide data preparation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103b1f9-f009-4639-86d0-26c9657781ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chapter 2: Why Data Preparation is So Important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297f978-286c-4aad-93c6-54d17ce612d0",
   "metadata": {},
   "source": [
    "#### Tutorial Overview\n",
    "- Highlights the critical role of data preparation in predictive modeling projects.\n",
    "- Emphasizes the need for transforming raw data into a suitable form for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9ebb2-68ec-4a5b-b193-eb581ff3dde9",
   "metadata": {},
   "source": [
    "#### What Is Data in Machine Learning\n",
    "- Structured data consists of rows (examples) and columns (features).\n",
    "- Each example has input and output elements relevant to the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4d2ab-e23f-4f45-9b02-e4ee2ba72700",
   "metadata": {},
   "source": [
    "#### Raw Data Must Be Prepared\n",
    "- Raw data often contains noise, errors, and complex relationships that need to be addressed.\n",
    "- Preparing data involves correcting these issues to make it suitable for machine learning algorithms.\n",
    "- Machine Learning Algorithms Expect Numbers\n",
    "- Machine Learning Algorithms Have Requirements\n",
    "    + The relationship between data and algorithms is reciprocal. Algorithms have specific expectations for data, necessitating appropriate data preparation to meet these requirements. Meanwhile, the characteristics of the data can inform which algorithms might be most effective.\n",
    "- Model Performance Depends on Data:\n",
    "    + Complex Data: Raw data contains compressed complex nonlinear relationships that\n",
    "may need to be exposed\n",
    "    + Messy Data: Raw data contains statistical noise, errors, missing values, and conflicting\n",
    "examples.\n",
    "- We can think about getting the most out of our predictive modeling project in two ways:\n",
    "    + focus on the model\n",
    "        + We could minimally prepare the raw data and begin modeling. This puts full onus on the model to tease out the relationships in the data and learn the mapping function from inputs to outputs as best it can. This may be a reasonable path through a project and may require a large dataset and a flexible and powerful machine learning algorithm with few expectations, such as random forest or gradient boosting.\n",
    "    + focus on the data\n",
    "        + Alternately, we could push the onus back onto the data and the data preparation process. This requires that each row of data best expresses the information content of the data for modeling. Just like denormalization of data in a relational database to rows and columns, data preparation can denormalize the complex structure inherent in each single observation. This is also a reasonable path. It may require more knowledge of the data than is available but allows good or even best modeling performance to be achieved almost irrespective of the machine learning algorithm used.\n",
    "    + Often a balance between these approaches is pursued on any given project. That is both exploring powerful and flexible machine learning algorithms and using data preparation to best expose the structure of the data to the learning algorithms. This is all to say, data preprocessing is a path to better data, and in turn, better model performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3dd5a3-04cf-45e1-be5c-54a0b9d8eb79",
   "metadata": {},
   "source": [
    "#### Predictive Modeling Is Mostly Data Preparation\n",
    "- With standardized machine learning algorithms, the majority of the effort in a project is spent on preparing the unique data for modeling.\n",
    "- Proper data preparation is essential for achieving reliable and accurate predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32548a98-55b2-4c07-b797-0d90fb36006e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chapter 3: Tour of Data Preparation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ae864-b3df-4b23-8094-d12047929cd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Tutorial Overview\n",
    "- Introduction to various data preparation tasks.\n",
    "- Overview of common techniques and their importance in a machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c99b9a-6b0b-4ab8-acaa-5e6dd79a644b",
   "metadata": {},
   "source": [
    "#### Common Data Preparation Tasks\n",
    "- **Data Cleaning**:\n",
    "  - Identifying and correcting mistakes or errors in the data.\n",
    "  - Handling missing values, outliers, and duplicates.\n",
    "![Data Cleaning](../Photos/1.png)\n",
    "- **Feature Selection**:\n",
    "  - Identifying the most relevant input variables for the prediction task.\n",
    "  - Removing irrelevant or redundant features to improve model performance.\n",
    "![Feature Selection](../Photos/2.png)\n",
    "- **Data Transforms**:\n",
    "  - Changing the scale, type, or distribution of variables.\n",
    "  - Common transforms include normalization, standardization, and encoding categorical variables.\n",
    "![Data Transforms](../Photos/3.png)\n",
    "![Data Transforms](../Photos/4.png)\n",
    "- **Feature Engineering**:\n",
    "  - Creating new variables from existing data to better represent the underlying structure of the problem.\n",
    "  - Includes techniques like polynomial features and interaction terms.\n",
    "- **Dimensionality Reduction**:\n",
    "  - Reducing the number of input variables by projecting data into a lower-dimensional space.\n",
    "  - Techniques include Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).\n",
    " ![Dimensionality Reduction](../Photos/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e7f44-9dd3-49d6-b1da-7fd2b25c296b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chapter 4: Data Preparation Without Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca750320-d1f8-4be4-aaf4-85e8550a327b",
   "metadata": {},
   "source": [
    "#### Tutorial Overview\n",
    "- Discusses the importance of avoiding data leakage during data preparation.\n",
    "- Presents techniques for properly preparing data without contaminating the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae7958-e931-40e2-81e0-836dbb78b3c2",
   "metadata": {},
   "source": [
    "#### Problem With Naive Data Preparation\n",
    "- Applying data transforms to the entire dataset before splitting into training and test sets can cause data leakage.\n",
    "- Data leakage leads to overly optimistic model performance estimates that do not generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee768426-c049-44fb-a18b-21059ccb2205",
   "metadata": {},
   "source": [
    "#### Data Preparation With Train and Test Sets\n",
    "- Ensuring data preparation steps are applied only to the training set.\n",
    "- The test set remains unseen by the model until the final evaluation to ensure a fair assessment of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48e256a-facc-48af-a9d6-8458f236576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.848\n"
     ]
    }
   ],
   "source": [
    "# naive approach to normalizing the data before splitting the data and evaluating the model\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# standardize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8915f631-94ff-4096-978a-7b4ccca4ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.152\n"
     ]
    }
   ],
   "source": [
    "# correct approach for normalizing the data after the data is split before the model is evaluated\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef348eb-2653-4f05-81a4-f5f9a4966dce",
   "metadata": {},
   "source": [
    "#### Data Preparation With k-fold Cross-Validation\n",
    "- Applying data preparation techniques within each fold during cross-validation.\n",
    "- Ensures that the validation data remains unseen during training to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b47a2d-7ea9-4ace-8141-2ccd94c9c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.300 (3.513)\n"
     ]
    }
   ],
   "source": [
    "# naive data preparation for model evaluation with k-fold cross-validation\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# standardize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf9997f-6274-46e0-8981-2f50aa8eac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.400 (3.489)\n"
     ]
    }
   ],
   "source": [
    "# correct data preparation for model evaluation with k-fold cross-validation\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=7)\n",
    "\n",
    "# define the pipeline\n",
    "steps = list()\n",
    "steps.append(('scaler', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99da10-ca7d-425e-bba9-efafd00cada0",
   "metadata": {},
   "source": [
    "+ As with the train-test example in the previous section, removing data leakage has resulted\n",
    "in a slight improvement in performance when our intuition might suggest a drop given that\n",
    "data leakage often results in an optimistic estimate of model performance. Nevertheless, the\n",
    "examples demonstrate that data leakage may impact the estimate of model performance and\n",
    "how to correct data leakage by correctly performing data preparation after the data is split."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
